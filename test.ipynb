{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from util import *\n",
    "import re\n",
    "import clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = np.load(\"./dataset/sketchrnn_angel.full.npz\", encoding='latin1', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"408.0\" version=\"1.1\" width=\"437.0\"><defs/><rect fill=\"white\" height=\"408.0\" width=\"437.0\" x=\"0\" y=\"0\"/><path d=\"M239.0,69.0 m-8.0,-10.0 l-22.0,-12.0 -12.0,-3.0 l-37.0,1.0 -10.0,20.0 l-2.0,28.0 11.0,15.0 l25.0,15.0 37.0,8.0 l20.0,-9.0 11.0,-10.0 l2.0,-25.0 -3.0,-6.0 l-16.0,-24.0 -5.0,-3.0 l-13.0,3.0 m-70.0,38.0 l7.0,32.0 m33.0,1.0 l6.0,36.0 m27.0,-33.0 l0.0,26.0 -3.0,6.0 m-26.0,-2.0 l-3.0,17.0 -17.0,42.0 l-60.0,96.0 -9.0,37.0 l-4.0,38.0 11.0,18.0 l24.0,9.0 19.0,0.0 l28.0,5.0 34.0,0.0 l66.0,-11.0 22.0,-15.0 l2.0,-6.0 -4.0,-86.0 l-10.0,-67.0 -24.0,-38.0 l-38.0,-41.0 m-52.0,16.0 l-24.0,-40.0 -58.0,-70.0 l-18.0,-15.0 -43.0,-25.0 l4.0,7.0 47.0,42.0 l13.0,24.0 -14.0,-1.0 l-27.0,-10.0 -4.0,5.0 l6.0,12.0 30.0,32.0 l-1.0,11.0 -13.0,4.0 l-49.0,0.0 1.0,5.0 l19.0,21.0 70.0,56.0 l16.0,8.0 12.0,2.0 l41.0,-2.0 m65.0,-70.0 l0.0,-7.0 16.0,-38.0 l15.0,-20.0 21.0,-17.0 l18.0,-10.0 57.0,-21.0 l51.0,-11.0 2.0,6.0 l-4.0,6.0 -54.0,34.0 l-8.0,8.0 -5.0,7.0 l33.0,2.0 -2.0,7.0 l-51.0,43.0 6.0,1.0 l18.0,-6.0 82.0,-38.0 l6.0,-1.0 2.0,7.0 l-77.0,69.0 -66.0,35.0 l-23.0,18.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_strokes(example_data['train'][100], factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_stroke_data(stroke_data: np.array):\n",
    "    # Going from [x, y, lift_pen] to [delta_x, delta_y, pen_on_paper, pen_off_paper, finished]\n",
    "    new_doodle = np.zeros((stroke_data.shape[0], 5))\n",
    "    \n",
    "    # Handling delta_x, delta_y\n",
    "    new_row = np.zeros((1, 3))\n",
    "    temp = np.vstack([new_row, stroke_data])\n",
    "    new_doodle[:, :2] = temp[1:, :2] - temp[:-1, :2]\n",
    "    \n",
    "    # Handling pen_on_paper and pen_off_paper\n",
    "    new_doodle[:, 2] = stroke_data[:, 2] == 0\n",
    "    new_doodle[:, 3] = stroke_data[:, 2] == 1\n",
    "\n",
    "    # Handling finished\n",
    "    new_doodle[-1, 2] = 0 \n",
    "    new_doodle[-1, 3] = 0 \n",
    "    new_doodle[-1, 4] = 1\n",
    "\n",
    "    return new_doodle\n",
    "\n",
    "def decode_stroke_data(stroke_data: np.array):\n",
    "    # Going from [delta_x, delta_y, pen_on_paper, pen_off_paper, finished] to [x, y, lift_pen]\n",
    "    new_doodle = np.zeros((stroke_data.shape[0], 3))\n",
    "    new_doodle[:, :2] = np.cumsum(stroke_data[:, :2], axis=0)    \n",
    "    new_doodle[:, 2] = np.logical_or(stroke_data[:, 3], stroke_data[:, 4])\n",
    "    return new_doodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP Model\n"
     ]
    }
   ],
   "source": [
    "class DoodleDataset(Dataset):\n",
    "    def __init__(self, data_dir: Path, split: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.data = {}\n",
    "        self.class_embeddings = {}\n",
    "        \n",
    "        print(\"Preprocessing Data:\")\n",
    "        for filepath in tqdm(self.data_dir.glob(\"*.npz\")):\n",
    "            class_name = self._extract_class_name(filepath)\n",
    "            class_data = self._extract_data(filepath, split)\n",
    "            self.data[class_name] = class_data\n",
    "        \n",
    "        print(\"Calculating CLIP Embeddings\")\n",
    "        class_names = list(self.data.keys())\n",
    "        tokenized_classnames = self.model.tokenize(class_names)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.encode_text(tokenized_classnames)\n",
    "        \n",
    "        for class_name, class_features in zip(class_names, text_features):\n",
    "            self.class_embeddings[class_name] = class_features\n",
    "\n",
    "        # Fancy preprocessing for faster indexing        \n",
    "        self.class_order_counts = [(class_name, len(class_data)) for class_name, class_data in self.data.items()]\n",
    "        self.class_order_count_cumsum = [0]\n",
    "        for _, class_count in self.class_order_counts:\n",
    "            self.class_order_count_cumsum.append(self.class_order_count_cumsum[-1] + class_count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(class_data) for _, class_data in self.data.items())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < self.class_order_count_cumsum[-1], f\"Index of {idx} is out of bounds, dataset has size {self.__len__()}\"\n",
    "        # [1, 2, 4, 3]\n",
    "        # [0, 1, 3, 7, 10]\n",
    "        class_idx = 0\n",
    "        while idx > self.class_order_count_cumsum[class_idx]:\n",
    "            class_idx += 1\n",
    "        \n",
    "        class_name = self.class_order_counts[class_idx][0]\n",
    "        in_class_idx = idx - self.class_order_count_cumsum[class_idx]\n",
    "        return (self.data[class_name][in_class_idx], self.class_embeddings[class_name])\n",
    "    \n",
    "    def get_class_name_embedding(self, class_name: str):\n",
    "        text_tokens = self.tokenizer(class_name)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_embedding = self.model.encode_text(text_tokens)\n",
    "        \n",
    "        # Normalize\n",
    "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
    "        return text_embedding\n",
    "\n",
    "    def _extract_class_name(self, file: Path):\n",
    "        # Example filename: `sketchrnn_apple.full.npz`\n",
    "        pattern = r\"sketchrnn_([^.]+)\\.full\\.npz\"\n",
    "        match = re.match(pattern, file.name)\n",
    "        assert match, f\"Regex for detecting classname failed on {file}\"        \n",
    "        return match.group(1)\n",
    "\n",
    "    def _extract_data(self, file: Path, split: str):\n",
    "        assert split in [\"train\", \"test\", \"valid\"], f\"Split {split} is not one of: train, test, valid!\"\n",
    "\n",
    "        raw_data = np.load(file, encoding='latin1', allow_pickle=True)[split]\n",
    "        encoded_doodles = []\n",
    "        for doodle in raw_data:\n",
    "            encoded_doodles.append(encode_stroke_data(doodle))\n",
    "        \n",
    "        return encoded_doodles\n",
    "    \n",
    "\n",
    "train_dataset = DoodleDataset(Path(\"./dataset\"), split=\"train\")\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=256)\n",
    "\n",
    "# test_dataset = DoodleDataset(Path(\"./dataset\"), split=\"test\")\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class StrokeEncoderMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(5, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (6.3.1)\n",
      "Requirement already satisfied: regex in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (4.66.4)\n",
      "Requirement already satisfied: wcwidth in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from ftfy) (0.2.13)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/hk/38612x253n1dsh0hrzz8lk9c0000gn/T/pip-req-build-bv07d7kh\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/hk/38612x253n1dsh0hrzz8lk9c0000gn/T/pip-req-build-bv07d7kh\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from clip==1.0) (23.2)\n",
      "Requirement already satisfied: regex in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from clip==1.0) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from clip==1.0) (4.66.4)\n",
      "Requirement already satisfied: torch in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from clip==1.0) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from clip==1.0) (0.17.2)\n",
      "Requirement already satisfied: wcwidth in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from torch->clip==1.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from torch->clip==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from torch->clip==1.0) (2024.9.0)\n",
      "Requirement already satisfied: numpy in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from torchvision->clip==1.0) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/yash.saxena/miniconda3/envs/qna/lib/python3.12/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369488 sha256=8ca48087ac1dc1b4b8314cab04ff648ad1af0056d5c768b2516c83e031f542de\n",
      "  Stored in directory: /private/var/folders/hk/38612x253n1dsh0hrzz8lk9c0000gn/T/pip-ephem-wheel-cache-jze_dp6l/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:43<00:00, 8.16MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0547, -0.0061,  0.0495,  ..., -0.6638, -0.1281, -0.4950],\n",
      "        [ 0.1447,  0.0225, -0.2909,  ..., -0.4472, -0.3420,  0.1798],\n",
      "        [ 0.1981, -0.2040, -0.1533,  ..., -0.4514, -0.5664,  0.0596]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "print(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
