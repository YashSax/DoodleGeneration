{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from util import *\n",
    "import re\n",
    "import clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = np.load(\"./dataset/sketchrnn_angel.full.npz\", encoding='latin1', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"504.0\" version=\"1.1\" width=\"668.0\"><defs/><rect fill=\"white\" height=\"504.0\" width=\"668.0\" x=\"0\" y=\"0\"/><path d=\"M412.0,30.0 m-17.0,-5.0 l-12.0,0.0 -14.0,5.0 l-16.0,14.0 -15.0,19.0 l-11.0,23.0 -3.0,32.0 l2.0,9.0 6.0,6.0 l21.0,-2.0 13.0,-11.0 l12.0,-19.0 13.0,-32.0 l0.0,-19.0 -6.0,-5.0 l-22.0,-3.0 m-33.0,84.0 l-2.0,20.0 -16.0,50.0 l-76.0,187.0 -18.0,56.0 l-3.0,18.0 5.0,4.0 l51.0,-13.0 62.0,-1.0 l53.0,7.0 30.0,9.0 l26.0,1.0 7.0,-3.0 l19.0,-25.0 11.0,-33.0 l1.0,-51.0 -3.0,-15.0 l-17.0,-43.0 -44.0,-52.0 l-34.0,-60.0 -12.0,-37.0 l-6.0,-41.0 0.0,-37.0 m-75.0,371.0 l-6.0,4.0 -4.0,14.0 l-4.0,42.0 8.0,6.0 l14.0,0.0 9.0,-12.0 l2.0,-20.0 -13.0,-23.0 m100.0,9.0 l-5.0,-2.0 -8.0,3.0 l-6.0,8.0 -9.0,32.0 l2.0,16.0 7.0,4.0 l19.0,-2.0 16.0,-23.0 l5.0,-23.0 -1.0,-15.0 l-11.0,-7.0 m-109.0,-209.0 l-46.0,-70.0 -31.0,-41.0 l-18.0,-14.0 -9.0,-3.0 l-34.0,-2.0 -41.0,16.0 l-17.0,12.0 -12.0,14.0 l-16.0,22.0 -22.0,42.0 l-23.0,56.0 -1.0,24.0 l7.0,-2.0 38.0,-45.0 l26.0,-19.0 8.0,1.0 l4.0,20.0 5.0,9.0 l10.0,5.0 15.0,0.0 l25.0,-8.0 28.0,1.0 l33.0,26.0 25.0,8.0 l38.0,2.0 m105.0,-57.0 l43.0,-45.0 27.0,-21.0 l16.0,-8.0 27.0,-10.0 l15.0,-2.0 63.0,0.0 l14.0,6.0 34.0,38.0 l47.0,105.0 5.0,22.0 l-8.0,3.0 -16.0,-8.0 l-20.0,-33.0 -10.0,-29.0 l-6.0,-5.0 -12.0,2.0 l-43.0,36.0 -5.0,-2.0 l-4.0,-19.0 -11.0,-1.0 l-53.0,25.0 -19.0,-2.0 l-11.0,-10.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_strokes(example_data['train'][0], factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_stroke_data(stroke_data: np.array):\n",
    "    # Going from [x, y, lift_pen] to [delta_x, delta_y, pen_on_paper, pen_off_paper, finished]\n",
    "    new_doodle = np.zeros((stroke_data.shape[0], 5))\n",
    "    \n",
    "    # Handling delta_x, delta_y\n",
    "    new_row = np.zeros((1, 3))\n",
    "    temp = np.vstack([new_row, stroke_data])\n",
    "    new_doodle[:, :2] = temp[1:, :2] - temp[:-1, :2]\n",
    "    \n",
    "    # Handling pen_on_paper and pen_off_paper\n",
    "    new_doodle[:, 2] = stroke_data[:, 2] == 0\n",
    "    new_doodle[:, 3] = stroke_data[:, 2] == 1\n",
    "\n",
    "    # Handling finished\n",
    "    new_doodle[-1, 2] = 0 \n",
    "    new_doodle[-1, 3] = 0 \n",
    "    new_doodle[-1, 4] = 1\n",
    "\n",
    "    return new_doodle\n",
    "\n",
    "def decode_stroke_data(stroke_data: np.array):\n",
    "    # Going from [delta_x, delta_y, pen_on_paper, pen_off_paper, finished] to [x, y, lift_pen]\n",
    "    new_doodle = np.zeros((stroke_data.shape[0], 3))\n",
    "    new_doodle[:, :2] = np.cumsum(stroke_data[:, :2], axis=0)    \n",
    "    new_doodle[:, 2] = np.logical_or(stroke_data[:, 3], stroke_data[:, 4])\n",
    "    return new_doodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (130272, 30) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     81\u001b[39m             blocks.append(block)\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array(blocks)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m train_dataset = \u001b[43mDoodleDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m train_dataloader = DataLoader(train_dataset, batch_size=\u001b[32m256\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# test_dataset = DoodleDataset(Path(\"./dataset\"), split=\"test\")\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# test_dataloader = DataLoader(test_dataset, batch_size=256)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mDoodleDataset.__init__\u001b[39m\u001b[34m(self, data_dir, split, max_len)\u001b[39m\n\u001b[32m     10\u001b[39m     class_name = \u001b[38;5;28mself\u001b[39m._extract_class_name(filepath)\n\u001b[32m     11\u001b[39m     class_data = \u001b[38;5;28mself\u001b[39m._extract_data(filepath, split)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     blocks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_break_into_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_data\u001b[49m\u001b[43m)\u001b[49m            \n\u001b[32m     13\u001b[39m     \u001b[38;5;28mself\u001b[39m.data[class_name] = blocks.reshape(-\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCalculating CLIP Embeddings\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mDoodleDataset._break_into_blocks\u001b[39m\u001b[34m(self, doodle)\u001b[39m\n\u001b[32m     80\u001b[39m     block = doodle[i:i + \u001b[38;5;28mself\u001b[39m.max_len]\n\u001b[32m     81\u001b[39m     blocks.append(block)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (130272, 30) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "class DoodleDataset(Dataset):\n",
    "    def __init__(self, data_dir: Path, split: str, max_len):\n",
    "        self.data_dir = data_dir\n",
    "        self.max_len = max_len\n",
    "        self.data = {}\n",
    "        self.class_embeddings = {}\n",
    "        \n",
    "        print(\"Preprocessing Data:\")\n",
    "        for filepath in tqdm(self.data_dir.glob(\"*.npz\")):\n",
    "            class_name = self._extract_class_name(filepath)\n",
    "            class_data = self._extract_data(filepath, split)\n",
    "            blocks = self._break_into_blocks(class_data)            \n",
    "            self.data[class_name] = blocks\n",
    "        \n",
    "        print(\"Calculating CLIP Embeddings\")\n",
    "        class_names = list(self.data.keys())\n",
    "        tokenized_classnames = clip.tokenize(class_names)\n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.encode_text(tokenized_classnames.to(device))\n",
    "        \n",
    "        for class_name, class_features in zip(class_names, text_features):\n",
    "            self.class_embeddings[class_name] = class_features\n",
    "\n",
    "        # Fancy preprocessing for faster indexing        \n",
    "        self.class_order_counts = [(class_name, len(class_data)) for class_name, class_data in self.data.items()]\n",
    "        self.class_order_count_cumsum = [0]\n",
    "        for _, class_count in self.class_order_counts:\n",
    "            self.class_order_count_cumsum.append(self.class_order_count_cumsum[-1] + class_count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(class_data) for _, class_data in self.data.items())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < self.class_order_count_cumsum[-1], f\"Index of {idx} is out of bounds, dataset has size {self.__len__()}\"\n",
    "        \n",
    "        # Binary search would be slower than linear search for the amount of classes we have.\n",
    "        class_idx = -1\n",
    "        while idx > self.class_order_count_cumsum[class_idx]:\n",
    "            class_idx += 1\n",
    "        \n",
    "        class_name = self.class_order_counts[class_idx][0]\n",
    "        in_class_idx = idx - self.class_order_count_cumsum[class_idx]\n",
    "        return (self.data[class_name][in_class_idx], self.class_embeddings[class_name])\n",
    "\n",
    "    def _extract_class_name(self, file: Path):\n",
    "        # Example filename: `sketchrnn_apple.full.npz`\n",
    "        pattern = r\"sketchrnn_([^.]+)\\.full\\.npz\"\n",
    "        match = re.match(pattern, file.name)\n",
    "        assert match, f\"Regex for detecting classname failed on {file}\"        \n",
    "        return match.group(1)\n",
    "\n",
    "    def _extract_data(self, file: Path, split: str):\n",
    "        assert split in [\"train\", \"test\", \"valid\"], f\"Split {split} is not one of: train, test, valid!\"\n",
    "\n",
    "        raw_data = np.load(file, encoding='latin1', allow_pickle=True)[split]\n",
    "        encoded_doodles = []\n",
    "        for doodle in raw_data:\n",
    "            encoded_doodles.append(encode_stroke_data(doodle))\n",
    "        \n",
    "        return encoded_doodles\n",
    "    \n",
    "    def _break_into_blocks(self, doodle: np.array) -> np.array:\n",
    "        \"\"\"Break a doodle into blocks of size self.max_len with stride 1.\n",
    "        \n",
    "        Args:\n",
    "            doodle (np.array): Array of shape (num_strokes, 5) containing stroke data\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Array of shape (num_blocks, max_len, 5) containing overlapping blocks\n",
    "        \"\"\"\n",
    "        if len(doodle) < self.max_len:\n",
    "            # Padded tokens should indicate that the doodle has completed\n",
    "            padded = np.zeros((self.max_len, 5))\n",
    "            padded[:len(doodle)] = doodle\n",
    "            padded[len(doodle):, -1] = 1\n",
    "            return np.array([padded])\n",
    "        \n",
    "        \n",
    "        blocks = []\n",
    "        for i in range(len(doodle) - self.max_len + 1):\n",
    "            block = doodle[i:i + self.max_len]\n",
    "            blocks.append(block)\n",
    "        \n",
    "        return np.array(blocks)\n",
    "\n",
    "\n",
    "train_dataset = DoodleDataset(Path(\"./dataset\"), split=\"train\", max_len=30)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256)\n",
    "\n",
    "# test_dataset = DoodleDataset(Path(\"./dataset\"), split=\"test\")\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class StrokeEncoderMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(5, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 148)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max(len(train_dataset[i][0]) for i in range(len(train_dataset)))\n",
    "min_len = min(len(train_dataset[i][0]) for i in range(len(train_dataset)))\n",
    "\n",
    "min_len, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DoodleGeneration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
